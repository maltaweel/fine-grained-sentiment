{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_transformers import BertTokenizer, cached_path\n",
    "from training.transformer_utils.model import TransformerWithClfHeadAndAdapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model and config dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"models/transformer\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = torch.load(cached_path(os.path.join(model_path, \"model_training_args.bin\")))\n",
    "model = TransformerWithClfHeadAndAdapters(config[\"config\"],\n",
    "                                          config[\"config_ft\"]).to(device)\n",
    "state_dict = torch.load(cached_path(os.path.join(model_path, \"model_weights.pth\")),\n",
    "                        map_location=device)\n",
    "\n",
    "model.load_state_dict(state_dict)   # Load model state dict\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased', do_lower_case=False)  # Load tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define special tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_token = tokenizer.vocab['[CLS]']  # classifier token\n",
    "pad_token = tokenizer.vocab['[PAD]']  # pad token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show maximum sequence length for trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = config['config'].num_max_positions  # Max length from trained model\n",
    "max_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(inputs):\n",
    "    # Encode text as IDs using the BertTokenizer\n",
    "    return list(tokenizer.convert_tokens_to_ids(o) for o in inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Effective but too-tepid biopic.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Effective but too-tepid biopic.\"\n",
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step converts the text's tokens into IDs that can be used for model training/inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Effect', '##ive', 'but', 'too', '-', 'te', '##pid', 'bio', '##pic', '.']\n",
      "[27007, 2109, 1133, 1315, 118, 21359, 25786, 25128, 20437, 119, 101]\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer.tokenize(text)\n",
    "if len(inputs) >= max_length:\n",
    "    inputs = inputs[:max_length - 1]\n",
    "ids = encode(inputs) + [clf_token]\n",
    "print(inputs)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate\n",
    "When evaluating the model against a test sample, dropout and gradient backpropagation must be disabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval(); # Disable dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():   # Disable backprop\n",
    "    tensor = torch.tensor(ids, dtype=torch.long).to(device)\n",
    "    tensor_reshaped = tensor.reshape(1, -1)\n",
    "    tensor_in = tensor_reshaped.transpose(0, 1).contiguous()  # to shape [seq length, 1]\n",
    "    logits = model(tensor_in,\n",
    "                   clf_tokens_mask=(tensor_in == clf_token),\n",
    "                   padding_mask=(tensor_reshaped == pad_token))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor inputs to the model are reshaped and the appropriate classification token masks and padding masks applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids in tensor form:  tensor([27007,  2109,  1133,  1315,   118, 21359, 25786, 25128, 20437,   119,\n",
      "          101])\n",
      "tensor reshaped:  tensor([[27007,  2109,  1133,  1315,   118, 21359, 25786, 25128, 20437,   119,\n",
      "           101]])\n",
      "tensor input to model:  tensor([[27007],\n",
      "        [ 2109],\n",
      "        [ 1133],\n",
      "        [ 1315],\n",
      "        [  118],\n",
      "        [21359],\n",
      "        [25786],\n",
      "        [25128],\n",
      "        [20437],\n",
      "        [  119],\n",
      "        [  101]])\n"
     ]
    }
   ],
   "source": [
    "print(\"ids in tensor form: \", tensor)\n",
    "print(\"tensor reshaped: \", tensor_reshaped)\n",
    "print(\"tensor input to model: \", tensor_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert logits to class probabilities\n",
    "The logits are input to a softmax function, detached from the computation graph, and converted to probabilities stored in a numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, _ = torch.max(logits, 0)\n",
    "val = F.softmax(val, dim=0).detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw logits:  tensor([[-0.4908,  2.7907,  2.2662, -0.6466, -3.1275]]) <class 'torch.Tensor'>\n",
      "Class probabilities:  [0.02257462 0.60087246 0.3556181  0.0193185  0.00161635] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw logits: \", logits, type(logits))\n",
    "print(\"Class probabilities: \", val, type(val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert to class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train the transformer in PyTorch we zero-indexed the labels.\n",
    "Now we increment the predicted most likely label by 1 to match with the original class label definition for SST-5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class prediction for text example:  2\n"
     ]
    }
   ],
   "source": [
    "pred = int(val.argmax()) + 1\n",
    "print(\"Class prediction for text example: \", pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
